---
title: "Occupancy model from eBird"
author: "Ben Goldstein"
date: "2/10/2020"
output: html_document
---

```{r setup, include=FALSE}
library(unmarked)
library(raster)
library(tidyverse)
library(prism)
```

Our four species are: verdin, black-tailed gnatcatcher, crissal thrasher, and least Bell's vireo.

```{r}
target_common_names <- c("Verdin", "Black-tailed_Gnatcatcher", "Crissal_Thrasher", "Bells_Vireo")
```


First, read in the data we have. Data come from 4 sources: eBird, iNaturalist, GBIF, and NABBS.

```{r message = F}
# eBird data
ebird_CA_obs <- read_csv("eBird_data/ebird_target_spec_obs_CA.csv") %>% 
                  filter(name_clean %in% target_common_names)
ebird_CA_checklists <- read_csv("eBird_data/ebird_target_spec_checklists_CA.csv")
ebird_NV_obs <- read_csv("eBird_data/ebird_target_spec_obs_NV.csv") %>% 
                  filter(name_clean %in% target_common_names)
ebird_NV_checklists <- read_csv("eBird_data/ebird_target_spec_checklists_NV.csv")
ebird_AZ_obs <- read_csv("eBird_data/ebird_target_spec_obs_AZ.csv") %>% 
                  filter(name_clean %in% target_common_names)
ebird_AZ_checklists <- read_csv("eBird_data/ebird_target_spec_checklists_AZ.csv")
ebird_UT_obs <- read_csv("eBird_data/ebird_target_spec_obs_UT.csv") %>% 
                  filter(name_clean %in% target_common_names)
ebird_UT_checklists <- read_csv("eBird_data/ebird_target_spec_checklists_UT.csv")

# Get unique species scientific - common name pairs
species_names <- ebird_CA_obs %>% 
                 dplyr::select(name_clean, SCIENTIFIC.NAME) %>% 
                 distinct(name_clean, SCIENTIFIC.NAME)

# Get ebird protocol codes
protocol_codes <- read_csv("eBird_data/ebird_protocol_codes.csv")
```



# Setting up an occupancy model

The model is:

$$y_{ij} \sim Bernoulli(z_i p_{ij})$$
$$z_i \sim Bernoulli(\psi_i)$$

$$logit(\psi_i)=X_{i(occ)}^T\beta$$
$$logit(p_{ij}) = W_{ij(det)}^T\gamma$$

- $y_{ij}$ is the *j*th visit at the *i*th site. It takes values {0, 1}.
- $z_i$ is the latent (unmeasured) "true" occupancy status of site *i*. It also has values {0, 1}.
- $p_{ij}$ is the probability of detecting an individual during the *j*th visit to the *i*th site **given the site is occupied**.
- $\psi_i$ is the probability that site *i* is in fact occupied. It can also be thought of as an occupancy "rate" when taken across sites.
- $W_{ij}^l$ are site-level covariates corresponding to the detection process. These might include time of day, day of year, etc. $W_{ij}^l$ corresponds to the value of the *l*th covariate of detection for the *j*th visit at the *i*th site.
- $X_{ij}^l$ are site-level covariates corresponding to the occupancy probability. These might include precipitation, elevation, etc. They do not change from visit to visit. $X_{ij}^l$ corresponds to the value of the *l*th covariate of occupancy for the *j*th visit at the *i*th site.

### What is a site?

Identifying the detection rate in the occupancy model is aided by (requires? I forget) having repeated visits to a site, which have the same underlying occupancy status.

First, let's get a sense of how the data are arranged spatially.

```{r}
all_checklist_info <- bind_rows(ebird_AZ_checklists, ebird_CA_checklists, 
                                ebird_NV_checklists, ebird_UT_checklists) %>% 
                      filter(ALL.SPECIES.REPORTED == 1)

distinct_locations_test <- all_checklist_info %>%
                      count(LATITUDE, LONGITUDE)

sum(distinct_locations_test$n > 1)
sum(distinct_locations_test$n[distinct_locations_test$n > 1])
```

If we only consider those locations with two or more "visits," we're left with a solid 173,000 sites. That's a lot of info to work with.

Some first thoughts on how we could refine think about "sites":

- We could round coordinates, aggregating sites to a tunable level of precision.
- It may be that we want to split up time into our definition of site. I don't think this is a great idea for now, as this would get us into dynamic occupancy model territory, although that may well be something we end up wanting to do. It's an established framework for thinking about range/occupancy changes in birds.


Let's try my rounding idea.

```{r}
all_checklist_info$lat_rd <- round(all_checklist_info$LATITUDE, 2)
all_checklist_info$lon_rd <- round(all_checklist_info$LONGITUDE, 2)

distinct_locations_rd <- all_checklist_info %>%
                         count(lat_rd, lon_rd)

sum(distinct_locations_rd$n > 1)
sum(distinct_locations_rd$n[distinct_locations_rd$n > 1])

```

Even rounding to 2 decimal points, which is very imprecise, we're only adding a bit of visit-level info. I think rounding is not a good idea.

Something I'm now thinking is that this might be too much data for an unmarked fit. It could take several hours to fit the model. I think that's ok, though. Also, given that we potentially have too *much* data, we can do some quality control filtering. 

Along those lines, what's the breakdown of protocols?

```{r}
all_checklist_info %>% 
  count(PROTOCOL.CODE, sort = T) %>% 
  left_join(protocol_codes)
```


### Gathering data

**Quality control filtering**. We could:

- Drop any observations other than select protocols (P22 and P21 are our targets)
- Drop observations made outside the breeding season (we'll call the breeding season March-July for now but we can refine this with some expert input)
- Drop data with unreported checklist covariates

Let's do all three of these and see how many observations & sites we have left.

```{r}

good_checklist_info <- 
  all_checklist_info %>% 
  mutate(year = lubridate::year(OBSERVATION.DATE),
         month = lubridate::month(OBSERVATION.DATE),
         mday = lubridate::mday(OBSERVATION.DATE),
         yday = lubridate::yday(OBSERVATION.DATE),
         tod = lubridate::hour(TIME.OBSERVATIONS.STARTED) * 60 + 
               lubridate::minute(TIME.OBSERVATIONS.STARTED)) %>% 
  filter(PROTOCOL.CODE %in% c("P22", "P21") &
         month %in% 3:7 &
         !is.na(OBSERVATION.DATE) &
         !is.na(TIME.OBSERVATIONS.STARTED) &
         !is.na(PROTOCOL.CODE) &
         !is.na(DURATION.MINUTES) &
         !is.na(LATITUDE) &
         !is.na(LONGITUDE))

good_checklist_info$EFFORT.DISTANCE.KM[is.na(good_checklist_info$EFFORT.DISTANCE.KM) & 
                                       good_checklist_info$PROTOCOL.CODE == "P21"] <- 0

distinct_locations_test <- good_checklist_info %>%
                      count(LATITUDE, LONGITUDE)

sum(distinct_locations_test$n > 1)
sum(distinct_locations_test$n[distinct_locations_test$n > 1])
```

We still have quite a bit of data, so let's roll with this.

Let's finally grab the observation data. We'll do this for a single species first. I'll arbitrarily select the Black-tailed Gnatcatcher as a focal species.

(One interesting thing to note is that if we stick with eBird data we can use this procedure for any bird of interest.)

```{r}
all_obs <- bind_rows(ebird_NV_obs, ebird_CA_obs, ebird_AZ_obs, ebird_UT_obs)
btg_obs <- all_obs %>% filter(name_clean == "Black-tailed_Gnatcatcher")

good_checklist_info$observed <- good_checklist_info$SAMPLING.EVENT.IDENTIFIER %in% btg_obs$SAMPLING.EVENT.IDENTIFIER

sum(!(btg_obs$SAMPLING.EVENT.IDENTIFIER %in% good_checklist_info$SAMPLING.EVENT.IDENTIFIER)) /
  nrow(btg_obs)
```

We lost about 60% of our IDs when we filtered observations. Hopefully this won't be a problem.

Let's take a look at the spatial arrangement of IDs:

```{r}
us <- map_data("state", region = c("Nevada", "California", "Arizona", "Utah"))

ggplot() +
  geom_map(data = us, map = us, aes(long, lat, map_id = region), 
           col = "black", fill = "white") +
  geom_point(data = good_checklist_info[sample(1:nrow(good_checklist_info), size = 300000), ], 
             aes(LONGITUDE, LATITUDE, col = observed, alpha = observed), cex = 0.2) + coord_fixed() +
             scale_alpha_discrete(c(0.1, 1))
```


What if we spatially crop our data to make it more manageable?

```{r}
cropped_checklists <- good_checklist_info %>% 
                      filter(LONGITUDE < -109 & LONGITUDE > -120 &
                             LATITUDE < 40 & LATITUDE > 30)

ggplot() +
  geom_map(data = us, map = us, aes(long, lat, map_id = region), 
           col = "black", fill = "white") +
  geom_point(data = cropped_checklists[sample(1:nrow(cropped_checklists), size = 100000), ], 
             aes(LONGITUDE, LATITUDE, col = observed, alpha = observed), cex = 0.33) + coord_fixed() +
             scale_alpha_discrete(c(0.1, 1))

```




### Gather site-level environmental covariates

(For now I'm just going to get one)

Set up a points object for our unique locations

```{r}
distinct_locations <- cropped_checklists %>%
                      count(LATITUDE, LONGITUDE, sort = T) %>% 
                      filter(n > 1) %>% 
                      mutate(id = row_number())

points <- SpatialPointsDataFrame(coords = distinct_locations[, c("LONGITUDE", "LATITUDE")],
                                 data = distinct_locations, 
                                 proj4string = CRS("+proj=longlat +datum=WGS84"))

```


Extract 4 climate variables from the PRISM climate dataset:

```{r}
if (!file.exists("climate_data/PRISM_proc_crop_raster_stack.grd")) {
  options(prism.path = "/Volumes/deValpineLab/TNC-DS421/climate_data/")
  get_prism_normals(type = "ppt", resolution = "800m", mon = 1:12, keepZip = TRUE)
  get_prism_normals(type = "tmean", resolution = "800m", mon = 1:12, keepZip = TRUE)
  get_prism_normals(type = "tmin", resolution = "800m", mon = 1:12, keepZip = TRUE)
  get_prism_normals(type = "tmax", resolution = "800m", mon = 1:12, keepZip = TRUE)
  
  raster_files <- list.files(path = "climate_data/", 
                             full.names = TRUE, recursive = T, pattern = "\\.bil$")
  
  precip_files <- raster_files[grepl("ppt", raster_files)]
  tmean_files <- raster_files[grepl("tmean", raster_files)]
  tmin_files <- raster_files[grepl("tmin", raster_files)]
  tmax_files <- raster_files[grepl("tmax", raster_files)]
  
  precip_rasters <- purrr::map(precip_files, raster)
  precip_rasters <- crop(stack(precip_rasters), extent(points) + 0.2)
  precip <- mean(precip_rasters)
  
  tmean_rasters <- purrr::map(tmean_files, raster)
  tmean_rasters <- crop(stack(tmean_rasters), extent(points) + 0.2)
  tmean <- mean(tmean_rasters)
  
  tmin_rasters <- purrr::map(tmin_files, raster)
  tmin_rasters <- crop(stack(tmin_rasters), extent(points) + 0.2)
  tmin <- min(tmin_rasters)
  # 
  tmax_rasters <- purrr::map(tmax_files, raster)
  tmax_rasters <- crop(stack(tmax_rasters), extent(points) + 0.2)
  tmax <- max(tmax_rasters)
  
  climate_rasters <- stack(precip,
                           tmean,
                           tmin,
                           tmax)
  
  stackSave(climate_rasters, "climate_data/PRISM_proc_crop_raster_stack.stk")
} else {
  climate_rasters <- stackOpen("climate_data/PRISM_proc_crop_raster_stack.stk")
}
env_covs <- raster::extract(climate_rasters, points)

distinct_locations$precip <- env_covs[,1]
distinct_locations$tmean <- env_covs[,2]
distinct_locations$tmin <- env_covs[,3]
distinct_locations$tmax <- env_covs[,4]
```


# Running the occupancy model

### Organize data for `unmarked`

The package `unmarked` likes its inputs in list form. The site-level covariates are a list of vectors. Visit-level covariates come in a list of matrices, indexed by [site, visit#]. Observation data also need to be formatted that way. Arranging the latter can be annoying.

First, we need to tag every checklist with the ID of its unique location.

```{r}
cropped_checklists <- left_join(cropped_checklists, 
                                distinct_locations[, c("LONGITUDE", "LATITUDE", "id")]) %>% 
                      filter(!is.na(id))
```

We need to make a matrix out of each observation covariate. I'll set up a function to do that.

```{r}
# Function
get_var_wide <- function(df, col, maxobs) {
  df <- df[, c("id", col)] %>% arrange(id)
  colnames(df)[2] <- "target_col"
  
  df %>% 
    group_by(id) %>% 
    mutate(visit = paste0("X", str_pad(1:n(), 5, pad = "0"))) %>%
    filter(visit <= paste0("X", str_pad(maxobs, 5, pad = "0"))) %>% 
    spread(key = visit, value = target_col) %>% 
    arrange(id) %>% 
    ungroup() %>% 
    dplyr::select(-id) %>% 
    as.matrix()
}

# Randomize the order of rows for the sake of cropping (to improve computation time during testing)
cropped_checklists_shuffled <- sample(cropped_checklists)

nsites <- 47343
nvisits <- 10

# Start getting matrices of each variable of interest
observed_mtx <- get_var_wide(cropped_checklists_shuffled, "observed", nvisits)
distance_mtx <- get_var_wide(cropped_checklists_shuffled, "EFFORT.DISTANCE.KM", nvisits)
tod_mtx <- get_var_wide(cropped_checklists_shuffled, "tod", nvisits)
doy_mtx <- get_var_wide(cropped_checklists_shuffled, "yday", nvisits)
protocol_mtx <- get_var_wide(cropped_checklists_shuffled, "PROTOCOL.CODE", nvisits)
duration_mtx <- get_var_wide(cropped_checklists_shuffled, "DURATION.MINUTES", nvisits)
```



Now we start putting the data frames together.

```{r}
site_covariates <- data.frame(precip = distinct_locations$precip,
                              tmean = distinct_locations$tmean,
                              tmax = distinct_locations$tmax,
                              tmin = distinct_locations$tmin,
                              latitude = distinct_locations$LATITUDE,
                              longitude = distinct_locations$LONGITUDE)[1:nsites,]
observation_covariates <- list(distance = distance_mtx[1:nsites,],
                               tod = tod_mtx[1:nsites,],
                               doy = doy_mtx[1:nsites,],
                               duration = duration_mtx[1:nsites,],
                               protocol = protocol_mtx[1:nsites,])

```

How does what we have correspond to the model?

- The object `observed_mtx` corresponds to $y_{ij}$.
- The matrices in the list `observation_covariates` are each $X_{ij}$, comprising $X_{ij}^l$
- The vectors in the list `site_covariates` are each $W_{i}$, comprising $W_{i}^l$

<!-- Michael wanted the following covariates: -->
<!-- - Mean precipitation -->
<!--     - Min/max precip -->
<!-- - Mean temp -->
<!--     - Min/max temp -->
<!-- - Land cover -->
<!-- - Distance from fresh water -->
<!-- - Pop density? -->
<!-- - Wetland/riparian vs desert shrubs? -->
<!-- - SouthWest ReGAP -->





```{r}
occu_frame <- unmarkedFrameOccu(y = observed_mtx, 
                                siteCovs = site_covariates, 
                                obsCovs = observation_covariates)

summary(occu_frame)
```


### Run the `unmarked` model

```{r}
occufit <- occu(~ distance + tod + tod^2 + doy + doy^2 + duration + protocol 
                ~ precip + tmean + tmin + tmax, 
                data = occu_frame)

summary(occufit)
```



### Put back onto space

```{r}
predicted_occu <- precip
values(predicted_occu) <- locfit::expit(-0.35426497 + -0.08966197 * values(precip))

```



# TODO: 

- Enforce a specific degree of lat/lon rounding
- Include more covariates
- Figure out goodness-of-fit check for the model
- Get Mexico data?
- Ask TNC folks for covariate ideas









